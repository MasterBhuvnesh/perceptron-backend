[
  {
    "id": "1",
    "title": "EDA",
    "description": "Exploratory Data Analysis (EDA) is a crucial first step in the data analysis pipeline. It involves a thorough investigation of a dataset to understand its underlying structure, patterns, and characteristics before applying any formal statistical modeling or machine learning algorithms. The philosophy behind EDA is to let the data 'speak for itself' through visual and quantitative means, allowing analysts to form hypotheses, detect anomalies, and understand the relationships between variables. It is an iterative process of asking questions, visualizing, transforming, and modeling to gain the deepest possible insights.",
    "theory": "The theoretical foundation of EDA was formally established by statistician John Tukey in the 1970s, who emphasized the importance of using graphical techniques to understand data beyond formal hypothesis testing. EDA is built on several key principles: a focus on visual representation to see patterns that summary statistics might miss; resistance, which prioritizes methods that are not overly influenced by outliers; and transparency, ensuring the data's story is clear. It involves two main types of analysis: univariate analysis, which examines the distribution, central tendency, and spread of a single variable (using tools like histograms, box plots, and summary statistics), and multivariate analysis, which explores the relationships and interactions between two or more variables (using scatter plots, correlation matrices, and pair plots). The ultimate goal is to uncover the underlying structure of the data, identify important variables, detect outliers and anomalies, test underlying assumptions for future modeling, and inform the selection of appropriate analytical techniques.",
    "image": "https://i.imgur.com/xyPZ4L1.png",
    "summary": "The initial process of analyzing data sets to summarize their main characteristics and uncover underlying patterns, often using visual methods."
  },
  {
    "id": "2",
    "title": "Linear Regression",
    "description": "Linear Regression is one of the most fundamental and widely used predictive modeling techniques in statistics and machine learning. It is used to model the relationship between a continuous target variable (dependent variable) and one or more predictor variables (independent variables). The core assumption is that this relationship can be approximated by a linear equation, meaning a change in a predictor leads to a proportional change in the target. Its simplicity, interpretability, and well-understood theory make it an essential tool for both inference—understanding the strength and nature of relationships—and prediction—forecasting future outcomes based on known inputs.",
    "theory": "The theoretical bedrock of Linear Regression is the concept of fitting a linear model to the observed data. The model for Simple Linear Regression, with one predictor, is expressed as: y = β₀ + β₁x + ε, where y is the dependent variable, x is the independent variable, β₀ is the y-intercept, β₁ is the slope coefficient (representing the change in y for a one-unit change in x), and ε is the error term (the difference between the observed and predicted values). For Multiple Linear Regression, the equation extends to include more predictors: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε. The most common method for finding the best-fitting line is Ordinary Least Squares (OLS). OLS works by minimizing the sum of the squared differences (the residuals) between the observed values and the values predicted by the linear model. This minimization provides the optimal values for the coefficients (β). Key assumptions for valid OLS regression include linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors. The model's goodness-of-fit is often evaluated using metrics like R-squared (the proportion of variance in the target explained by the predictors) and Adjusted R-squared.",
    "image": "https://i.imgur.com/LFqG28B.png",
    "summary": "A foundational statistical method for modeling the linear relationship between a dependent variable and one or more independent variables."
  },
  {
    "id": "3",
    "title": "Classification",
    "description": "Classification is a core supervised learning task in machine learning where the goal is to predict a discrete categorical label (or class) for a given input. Instead of predicting a continuous value as in regression, classification models assign inputs to predefined categories. These categories can be binary (e.g., spam vs. not spam, fraud vs. legitimate) or multiclass (e.g., identifying different species of iris flowers, recognizing handwritten digits 0-9). The model learns the patterns and decision boundaries that separate the classes from a labeled training dataset, where the correct answers are already known, and then applies this learned function to classify new, unseen data.",
    "theory": "The theory of classification revolves around the concept of learning a decision boundary that partitions the feature space into regions, each associated with a specific class. Different algorithms approach this problem in distinct ways. Logistic Regression, despite its name, is a linear classification algorithm that models the probability that a given input belongs to a particular class using the logistic function (sigmoid). It's foundational for understanding how linear models can be adapted for classification. Decision Trees take a hierarchical, tree-like approach, splitting the data based on feature values to create pure subsets. Ensemble methods like Random Forest combine the predictions of multiple decorrelated decision trees to improve accuracy and robustness, reducing the risk of overfitting. Support Vector Machines (SVMs) aim to find the optimal hyperplane that maximizes the margin between classes in a high-dimensional space. Neural Networks, particularly deep learning models, use interconnected layers of nodes to learn highly complex, non-linear decision boundaries. The performance of a classifier is rigorously evaluated using a confusion matrix and derived metrics such as accuracy, precision, recall, and the F1-score, which provide a nuanced view of its strengths and weaknesses across different classes.",
    "image": "https://i.imgur.com/8VpWQ3j.png",
    "summary": "A predictive modeling task where observations are categorized into discrete classes based on learned patterns from labeled data."
  }
]
